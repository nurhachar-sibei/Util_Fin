"""
PostgreSQLæ•°æ®ç®¡ç†ç³»ç»Ÿ
ç”¨äºç®€å•çš„æ•°æ®å­˜å‚¨å’Œç®¡ç†

Author: Nurhachar
Date: 2025
"""

import numpy as np
import pandas as pd
import psycopg2
import psycopg2.extras
import json
import logging
import time
from functools import wraps
from typing import Union, List, Dict, Optional, Any
from pathlib import Path
import warnings
from Util_Fin import logger_util

print("Easy Manager is running...")
# é…ç½®æ—¥å¿—
# é…ç½®æ—¥å¿—æ ¼å¼ï¼Œä½¿å…¶æ›´ç¬¦åˆç”¨æˆ·è¦æ±‚çš„æ ¼å¼
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s | %(levelname)s | %(message)s',
#     datefmt='%Y-%m-%d %H:%M:%S',
#     handlers=[
#         logging.FileHandler('datadeal.log', encoding='utf-8'),
#         logging.StreamHandler()
#     ]
# )

def function_timer(func):
    """
    å‡½æ•°è®¡æ—¶è£…é¥°å™¨
    """
    @wraps(func)
    def wrapper(*args, **kwargs):
        logger = logger_util.setup_logger('datadeal','./')
        logger.info(f'[Function: {func.__name__} started...]')
        start_time = time.time()
        
        try:
            result = func(*args, **kwargs)
            end_time = time.time()
            elapsed_time = end_time - start_time
            logger.info(f'[Function: {func.__name__} completed, elapsed time: {elapsed_time:.2f}s]')
            return result
        except Exception as e:
            end_time = time.time()
            elapsed_time = end_time - start_time
            logger.error(f'[Function: {func.__name__} failed after {elapsed_time:.2f}s, error: {str(e)}]')
            raise
    
    return wrapper


class EasyManager:
    """
    ç®€æ˜“PostgreSQLæ•°æ®ç®¡ç†ç±»
    æ”¯æŒåˆ›å»ºè¡¨æ ¼ã€æ’å…¥æ•°æ®ï¼ˆå»é‡ï¼‰ã€åˆ é™¤è¡¨æ ¼ã€å¯¼å…¥è¡¨æ ¼ç­‰æ“ä½œ
    """
    
    def __init__(self, 
                 database: str = "test_data_base",
                 user: str = "postgres", 
                 password: str = "cbw88982449",
                 host: str = "localhost",
                 port: str = "5432"):
        """
        åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        
        Args:
            database: æ•°æ®åº“å
            user: ç”¨æˆ·å
            password: å¯†ç 
            host: ä¸»æœºåœ°å€
            port: ç«¯å£å·
        """
        self.logger = logger_util.setup_logger("datadeal",'./')
        self.db_config = {
            'database': database,
            'user': user,
            'password': password,
            'host': host,
            'port': port
        }
        
        self.conn = None
        self.cursor = None
        self._connect()
    
    def _connect(self):
        """å»ºç«‹æ•°æ®åº“è¿æ¥"""
        try:
            self.conn = psycopg2.connect(**self.db_config)
            self.cursor = self.conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            self.logger.info(f"æ•°æ®åº“è¿æ¥æˆåŠŸ: {self.db_config['database']}")
        except Exception as e:
            self.logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
            raise
    
    def _ensure_connection(self):
        """ç¡®ä¿æ•°æ®åº“è¿æ¥æœ‰æ•ˆ"""
        try:
            self.cursor.execute("SELECT 1")
        except (psycopg2.OperationalError, psycopg2.InterfaceError):
            self.logger.warning("æ•°æ®åº“è¿æ¥å·²æ–­å¼€ï¼Œæ­£åœ¨é‡æ–°è¿æ¥...")
            self._connect()
    
    def _infer_column_type(self, series: pd.Series) -> str:
        """
        æ¨æ–­pandasåˆ—çš„PostgreSQLæ•°æ®ç±»å‹
        
        Args:
            series: pandas Series
            
        Returns:
            PostgreSQLæ•°æ®ç±»å‹å­—ç¬¦ä¸²
        """
        dtype = series.dtype
        
        # æ•°å€¼ç±»å‹
        if pd.api.types.is_integer_dtype(dtype):
            return "BIGINT"
        elif pd.api.types.is_float_dtype(dtype):
            return "DOUBLE PRECISION"
        # å¸ƒå°”ç±»å‹
        elif pd.api.types.is_bool_dtype(dtype):
            return "BOOLEAN"
        # æ—¥æœŸæ—¶é—´ç±»å‹
        elif pd.api.types.is_datetime64_any_dtype(dtype):
            return "TIMESTAMP"
        # å­—ç¬¦ä¸²ç±»å‹
        else:
            # æ£€æŸ¥æœ€å¤§é•¿åº¦
            max_length = series.astype(str).str.len().max()
            if pd.isna(max_length) or max_length == 0:
                return "TEXT"
            elif max_length <= 255:
                return f"VARCHAR({int(max_length * 1.5)})"  # ç•™ç‚¹ä½™é‡
            else:
                return "TEXT"
    
    def _clean_column_name(self, column_name: str) -> str:
        """
        æ¸…ç†åˆ—åï¼Œç¡®ä¿ç¬¦åˆSQLæ ‡å‡†
        
        Args:
            column_name: åŸå§‹åˆ—å
            
        Returns:
            æ¸…ç†åçš„åˆ—å
        """
        # æ›¿æ¢ç‰¹æ®Šå­—ç¬¦ä¸ºä¸‹åˆ’çº¿
        clean_name = column_name.replace('.', '_').replace('-', '_').replace(' ', '_')
        return clean_name
    
    @function_timer
    def create_table(self, table_name: str, dataframe: pd.DataFrame, 
                     overwrite: bool = False) -> bool:
        """
        åœ¨æ•°æ®åº“ä¸­åˆ›å»ºè¡¨æ ¼
        
        Args:
            table_name: è¡¨å
            dataframe: pandas DataFrame
            overwrite: æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„è¡¨
            
        Returns:
            bool: åˆ›å»ºæ˜¯å¦æˆåŠŸ
        """
        self._ensure_connection()
        
        try:
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            self.cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_name = %s
                );
            """, (table_name.split('.')[-1],))
            
            table_exists = self.cursor.fetchone()['exists']
            
            if table_exists and not overwrite:
                self.logger.warning(f"è¡¨ {table_name} å·²å­˜åœ¨ï¼Œä½¿ç”¨overwrite=Trueæ¥è¦†ç›–")
                return False
            
            if table_exists and overwrite:
                self.cursor.execute(f"DROP TABLE IF EXISTS {table_name} CASCADE")
                self.logger.info(f"å·²åˆ é™¤ç°æœ‰è¡¨ {table_name}")
            
            # åˆ›å»ºè¡¨ç»“æ„
            df = dataframe.copy()
            
            # å¤„ç†ç´¢å¼•ï¼šå¦‚æœç´¢å¼•æœ‰åç§°ä¸”ä¸æ˜¯é»˜è®¤çš„RangeIndexï¼Œå°†å…¶ä½œä¸ºåˆ—
            if df.index.name and not isinstance(df.index, pd.RangeIndex):
                df = df.reset_index()
            elif not isinstance(df.index, pd.RangeIndex):
                df.index.name = 'index'
                df = df.reset_index()
            
            # æ„å»ºåˆ—å®šä¹‰
            columns_sql = []
            for col in df.columns:
                col_type = self._infer_column_type(df[col])
                # æ¸…ç†åˆ—åï¼Œç¡®ä¿ç¬¦åˆSQLæ ‡å‡†
                clean_col = self._clean_column_name(col)
                columns_sql.append(f'"{clean_col}" {col_type}')
            
            create_sql = f"""
                CREATE TABLE {table_name} (
                    {', '.join(columns_sql)}
                )
            """
            
            self.cursor.execute(create_sql)
            self.conn.commit()
            
            self.logger.info(f"è¡¨ {table_name} åˆ›å»ºæˆåŠŸï¼ŒåŒ…å« {len(df.columns)} åˆ—")
            
            # æ’å…¥æ•°æ®
            self._insert_dataframe(table_name, df)
            
            return True
            
        except Exception as e:
            self.conn.rollback()
            import traceback
            self.logger.error(f"åˆ›å»ºè¡¨ {table_name} å¤±è´¥: {str(e)}")
            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return False
    
    def _insert_dataframe(self, table_name: str, df: pd.DataFrame):
        """
        å°†DataFrameæ’å…¥åˆ°è¡¨ä¸­
        
        Args:
            table_name: è¡¨å
            df: è¦æ’å…¥çš„DataFrame
        """
        if df.empty:
            self.logger.warning("DataFrameä¸ºç©ºï¼Œè·³è¿‡æ’å…¥")
            return
        
        # æ¸…ç†åˆ—å
        df_clean = df.copy()
        df_clean.columns = [self._clean_column_name(col) for col in df.columns]
        
        # å‡†å¤‡æ•°æ®
        columns = ', '.join([f'"{col}"' for col in df_clean.columns])
        placeholders = ', '.join(['%s'] * len(df_clean.columns))
        
        insert_sql = f"""
            INSERT INTO {table_name} ({columns})
            VALUES ({placeholders})
        """
        
        # è½¬æ¢æ•°æ®ä¸ºå…ƒç»„åˆ—è¡¨
        data_tuples = []
        for _, row in df_clean.iterrows():
            # å¤„ç†NaNå€¼
            row_data = tuple(None if pd.isna(x) else x for x in row)
            data_tuples.append(row_data)
        
        # æ‰¹é‡æ’å…¥
        psycopg2.extras.execute_batch(
            self.cursor, insert_sql, data_tuples, page_size=1000
        )
        self.conn.commit()
        
        self.logger.info(f"æˆåŠŸæ’å…¥ {len(data_tuples)} è¡Œæ•°æ®åˆ°è¡¨ {table_name}")
    
    def _get_table_columns(self, table_name: str) -> List[str]:
        """
        è·å–è¡¨çš„åˆ—ååˆ—è¡¨
        
        Args:
            table_name: è¡¨å
            
        Returns:
            åˆ—ååˆ—è¡¨
        """
        self.cursor.execute("""
            SELECT column_name 
            FROM information_schema.columns 
            WHERE table_name = %s
            ORDER BY ordinal_position
        """, (table_name.split('.')[-1],))
        
        return [row['column_name'] for row in self.cursor.fetchall()]
    
    @function_timer
    def add_columns(self, table_name: str, dataframe: pd.DataFrame, 
                    merge_on_index: bool = True) -> bool:
        """
        åœ¨è¡¨ä¸­å¢åŠ æ–°åˆ—ï¼ŒæŒ‰ç´¢å¼•åˆå¹¶æ•°æ®
        
        Args:
            table_name: è¡¨å
            dataframe: åŒ…å«æ–°åˆ—çš„ DataFrame
            merge_on_index: æ˜¯å¦åŸºäºç´¢å¼•åˆå¹¶ï¼ˆé»˜è®¤Trueï¼‰
            
        Returns:
            bool: æ·»åŠ æ˜¯å¦æˆåŠŸ
        """
        self._ensure_connection()
        
        try:
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            self.cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_name = %s
                );
            """, (table_name.split('.')[-1],))
            
            if not self.cursor.fetchone()['exists']:
                self.logger.error(f"è¡¨ {table_name} ä¸å­˜åœ¨")
                return False
            
            # è·å–ç°æœ‰åˆ—
            existing_columns = self._get_table_columns(table_name)
            self.logger.info(f"è¡¨ {table_name} ç°æœ‰åˆ—: {existing_columns}")
            
            # å¤„ç†DataFrame
            df = dataframe.copy()
            
            # å¤„ç†ç´¢å¼•
            index_name = None
            if df.index.name and not isinstance(df.index, pd.RangeIndex):
                index_name = df.index.name
                df = df.reset_index()
            elif not isinstance(df.index, pd.RangeIndex):
                index_name = 'index'
                df.index.name = index_name
                df = df.reset_index()
            
            # æ¸…ç†åˆ—å
            df.columns = [col.replace('.', '_').replace('-', '_').replace(' ', '_') 
                         for col in df.columns]
            if index_name:
                index_name = index_name.replace('.', '_').replace('-', '_').replace(' ', '_')
            
            # è¯†åˆ«æ–°åˆ—ï¼ˆæ’é™¤å·²å­˜åœ¨çš„åˆ—ï¼‰
            new_columns = [col for col in df.columns if col not in existing_columns]
            
            if not new_columns:
                self.logger.warning(f"æ²¡æœ‰æ–°åˆ—éœ€è¦æ·»åŠ åˆ°è¡¨ {table_name}")
                return True
            
            self.logger.info(f"è¯†åˆ«åˆ° {len(new_columns)} ä¸ªæ–°åˆ—: {new_columns}")
            
            # æ·»åŠ æ–°åˆ—åˆ°è¡¨ç»“æ„
            for col in new_columns:
                col_type = self._infer_column_type(df[col])
                alter_sql = f'ALTER TABLE {table_name} ADD COLUMN "{col}" {col_type}'
                self.cursor.execute(alter_sql)
                self.logger.info(f"æ·»åŠ åˆ— {col} (ç±»å‹: {col_type})")
            
            self.conn.commit()
            
            # æŒ‰ç´¢å¼•åˆå¹¶æ•°æ®
            if merge_on_index and index_name:
                # åŠ è½½ç°æœ‰æ•°æ®
                existing_df = self.load_table(table_name,limit=-10)
                
                if existing_df is not None and not existing_df.empty:
                    self.logger.info(f"æŒ‰ç´¢å¼•åˆ— '{index_name}' åˆå¹¶æ•°æ®")
                    
                    # åªä¿ç•™æ–°åˆ—å’Œç´¢å¼•åˆ—
                    df_new_cols = df[[index_name] + new_columns]
                    
                    # æ›´æ–°æ¯ä¸€è¡Œçš„æ–°åˆ—æ•°æ®
                    update_count = 0
                    for _, row in df_new_cols.iterrows():
                        index_value = row[index_name]
                        
                        # æ„å»ºUPDATEè¯­å¥
                        set_clause = ', '.join([f'"{col}" = %s' for col in new_columns])
                        update_sql = f"""
                            UPDATE {table_name}
                            SET {set_clause}
                            WHERE "{index_name}" = %s
                        """
                        
                        # å‡†å¤‡å‚æ•°
                        values = [None if pd.isna(row[col]) else row[col] for col in new_columns]
                        values.append(index_value)
                        self.cursor.execute(update_sql, values)
                        if self.cursor.rowcount > 0:
                            update_count += 1
                    
                    self.conn.commit()
                    self.logger.info(f"æˆåŠŸæ›´æ–° {update_count} è¡Œçš„æ–°åˆ—æ•°æ®")
            
            return True
            
        except Exception as e:
            self.conn.rollback()
            import traceback
            self.logger.error(f"æ·»åŠ åˆ—åˆ°è¡¨ {table_name} å¤±è´¥: {str(e)}")
            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            self.logger.error("è¯·ä¼˜å…ˆæ£€æŸ¥æ•°æ®æ ¼å¼é—®é¢˜,æ³¨:æ‰€æœ‰çš„æ—¶é—´æ ¼å¼éƒ½éœ€è¦pd.to_datatimeåæ–¹å¯å½•å…¥")
            return False
    
    @function_timer
    def insert_data(self, table_name: str, dataframe: pd.DataFrame, 
                    mode: str = 'skip') -> bool:
        """
        å‘è¡¨ä¸­æ’å…¥æ•°æ®ï¼Œæ”¯æŒå¤šç§é‡å¤æ•°æ®å¤„ç†æ¨¡å¼
        
        Args:
            table_name: è¡¨å
            dataframe: pandas DataFrame
            mode: é‡å¤æ•°æ®å¤„ç†æ¨¡å¼
                - 'skip': å¿½ç•¥é‡å¤æ•°æ®ï¼Œåªæ’å…¥æ–°æ•°æ®ï¼ˆé»˜è®¤ï¼‰
                - 'update': è¦†ç›–é‡å¤æ•°æ®ï¼ŒåŸºäºç´¢å¼•æ›´æ–°
                - 'append': ç›´æ¥è¿½åŠ ï¼Œä¸æ£€æŸ¥é‡å¤
            
        Returns:
            bool: æ’å…¥æ˜¯å¦æˆåŠŸ
        """
        self._ensure_connection()
        
        if mode not in ['skip', 'update', 'append']:
            self.logger.error(f"ä¸æ”¯æŒçš„æ¨¡å¼: {mode}ï¼Œè¯·ä½¿ç”¨ 'skip', 'update' æˆ– 'append'")
            return False
        
        try:
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            self.cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_name = %s
                );
            """, (table_name.split('.')[-1],))
            
            if not self.cursor.fetchone()['exists']:
                self.logger.error(f"è¡¨ {table_name} ä¸å­˜åœ¨ï¼Œè¯·å…ˆä½¿ç”¨create_tableåˆ›å»ºè¡¨")
                return False
            
            df = dataframe.copy()
            
            # å¤„ç†ç´¢å¼•
            index_name = None
            if df.index.name and not isinstance(df.index, pd.RangeIndex):
                index_name = df.index.name
                df = df.reset_index()
            elif not isinstance(df.index, pd.RangeIndex):
                index_name = 'index'
                df.index.name = index_name
                df = df.reset_index()
            
            # æ¸…ç†åˆ—å
            df.columns = [col.replace('.', '_').replace('-', '_').replace(' ', '_') 
                         for col in df.columns]
            if index_name:
                index_name = index_name.replace('.', '_').replace('-', '_').replace(' ', '_')
            
            # æ ¹æ®æ¨¡å¼å¤„ç†æ•°æ®
            if mode == 'append':
                # ç›´æ¥æ’å…¥ï¼Œä¸æ£€æŸ¥é‡å¤
                self.logger.info(f"ä½¿ç”¨ append æ¨¡å¼ï¼Œç›´æ¥æ’å…¥ {len(df)} è¡Œæ•°æ®")
                self._insert_dataframe(table_name, df)
                
            elif mode == 'skip':
                # å¿½ç•¥é‡å¤ç´¢å¼•çš„æ•°æ®ï¼Œåªæ’å…¥æ–°ç´¢å¼•çš„æ•°æ®
                if not index_name:
                    self.logger.error("skip æ¨¡å¼éœ€è¦æœ‰ç´¢å¼•åˆ—ï¼Œä½†æœªæ‰¾åˆ°ç´¢å¼•")
                    return False
                
                existing_df = self.load_table(table_name,limit=-80)
                
                if existing_df is not None and not existing_df.empty:
                    # æ£€æŸ¥ç´¢å¼•åˆ—æ˜¯å¦å­˜åœ¨
                    if index_name not in existing_df.columns:
                        self.logger.error(f"ç´¢å¼•åˆ— '{index_name}' ä¸å­˜åœ¨äºè¡¨ä¸­")
                        return False
                    
                    # åŸºäºç´¢å¼•æ‰¾å‡ºä¸é‡å¤çš„è¡Œ
                    existing_indices = set(existing_df[index_name].values)
                    new_indices = set(df[index_name].values)
                    
                    # åªä¿ç•™ç´¢å¼•ä¸é‡å¤çš„è¡Œ
                    indices_to_insert = new_indices - existing_indices
                    
                    # å¦‚æœæ²¡æœ‰æ–°æ•°æ®
                    if not indices_to_insert:
                        self.logger.info(f"æ²¡æœ‰æ–°çš„ç´¢å¼•æ•°æ®éœ€è¦æ’å…¥åˆ°è¡¨ {table_name}")
                        return True
                    
                    # è¿‡æ»¤å‡ºè¦æ’å…¥çš„æ•°æ®
                    df_to_insert = df[df[index_name].isin(indices_to_insert)]
                    
                    self.logger.info(f"ä½¿ç”¨ skip æ¨¡å¼ï¼ŒåŸºäºç´¢å¼•è¿‡æ»¤åæœ‰ {len(df_to_insert)} è¡Œæ–°æ•°æ®")
                    self._insert_dataframe(table_name, df_to_insert)
                else:
                    self.logger.info(f"è¡¨ä¸ºç©ºï¼Œæ’å…¥ {len(df)} è¡Œæ•°æ®")
                    self._insert_dataframe(table_name, df)
                    
            elif mode == 'update':
                # è¦†ç›–é‡å¤æ•°æ®ï¼ŒåŸºäºç´¢å¼•æ›´æ–°
                if not index_name:
                    self.logger.error("update æ¨¡å¼éœ€è¦æœ‰ç´¢å¼•åˆ—ï¼Œä½†æœªæ‰¾åˆ°ç´¢å¼•")
                    return False
                
                existing_df = self.load_table(table_name,limit=-80)
                
                if existing_df is None or existing_df.empty:
                    self.logger.info(f"è¡¨ä¸ºç©ºï¼Œç›´æ¥æ’å…¥ {len(df)} è¡Œæ•°æ®")
                    self._insert_dataframe(table_name, df)
                else:
                    # æ£€æŸ¥ç´¢å¼•åˆ—æ˜¯å¦å­˜åœ¨
                    if index_name not in existing_df.columns:
                        self.logger.error(f"ç´¢å¼•åˆ— '{index_name}' ä¸å­˜åœ¨äºè¡¨ä¸­")
                        return False
                    
                    # è·å–æ‰€æœ‰åˆ—ï¼ˆæ’é™¤ç´¢å¼•åˆ—ï¼‰
                    data_columns = [col for col in df.columns if col != index_name]
                    
                    # æ‰¾å‡ºéœ€è¦æ›´æ–°çš„è¡Œå’Œéœ€è¦æ’å…¥çš„è¡Œ
                    existing_indices = set(existing_df[index_name].values)
                    new_indices = set(df[index_name].values)
                    
                    indices_to_update = existing_indices & new_indices
                    indices_to_insert = new_indices - existing_indices
                    
                    update_count = 0
                    insert_count = 0
                    
                    # æ›´æ–°é‡å¤çš„è¡Œ
                    if indices_to_update:
                        self.logger.info(f"ä½¿ç”¨ update æ¨¡å¼ï¼Œæ›´æ–° {len(indices_to_update)} è¡Œ")
                        df_to_update = df[df[index_name].isin(indices_to_update)]
                        
                        for _, row in df_to_update.iterrows():
                            index_value = row[index_name]
                            
                            # æ„å»ºUPDATEè¯­å¥
                            set_clause = ', '.join([f'"{col}" = %s' for col in data_columns])
                            update_sql = f"""
                                UPDATE {table_name}
                                SET {set_clause}
                                WHERE "{index_name}" = %s
                            """
                            
                            # å‡†å¤‡å‚æ•°
                            values = [None if pd.isna(row[col]) else row[col] for col in data_columns]
                            values.append(index_value)
                            
                            self.cursor.execute(update_sql, values)
                            if self.cursor.rowcount > 0:
                                update_count += 1
                        
                        self.conn.commit()
                        self.logger.info(f"æˆåŠŸæ›´æ–° {update_count} è¡Œæ•°æ®")
                    
                    # æ’å…¥æ–°çš„è¡Œ
                    if indices_to_insert:
                        self.logger.info(f"æ’å…¥ {len(indices_to_insert)} è¡Œæ–°æ•°æ®")
                        df_to_insert = df[df[index_name].isin(indices_to_insert)]
                        self._insert_dataframe(table_name, df_to_insert)
                        insert_count = len(df_to_insert)
                    
                    self.logger.info(f"update æ¨¡å¼å®Œæˆ: æ›´æ–° {update_count} è¡Œ, æ’å…¥ {insert_count} è¡Œ")
            
            return True
            
        except Exception as e:
            self.conn.rollback()
            import traceback
            self.logger.error(f"æ’å…¥æ•°æ®åˆ°è¡¨ {table_name} å¤±è´¥: {str(e)}")
            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return False
    
    @function_timer
    def drop_table(self, table_name: str) -> bool:
        """
        åˆ é™¤è¡¨æ ¼
        
        Args:
            table_name: è¡¨å
            
        Returns:
            bool: åˆ é™¤æ˜¯å¦æˆåŠŸ
        """
        self._ensure_connection()
        
        try:
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            self.cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_name = %s
                );
            """, (table_name.split('.')[-1],))
            
            if not self.cursor.fetchone()['exists']:
                self.logger.warning(f"è¡¨ {table_name} ä¸å­˜åœ¨")
                return False
            
            # åˆ é™¤è¡¨
            self.cursor.execute(f"DROP TABLE {table_name} CASCADE")
            self.conn.commit()
            
            self.logger.info(f"æˆåŠŸåˆ é™¤è¡¨: {table_name}")
            return True
            
        except Exception as e:
            self.conn.rollback()
            import traceback
            self.logger.error(f"åˆ é™¤è¡¨ {table_name} å¤±è´¥: {str(e)}")
            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return False
    
    @function_timer
    def load_table(self, table_name: str, limit: Optional[int] = None,
                   order_by: Optional[str] = 'index', ascending: bool = True,
                   columns: Optional[List[str]] = None) -> Optional[pd.DataFrame]:
        """
        ä»æ•°æ®åº“å¯¼å…¥è¡¨æ ¼åˆ°Python
        
        Args:
            table_name: è¡¨å
            limit: é™åˆ¶è¿”å›è¡Œæ•°ï¼ˆå¯é€‰ï¼‰
                  - æ­£æ•°: è¿”å›å‰Nè¡Œ
                  - è´Ÿæ•°: è¿”å›åNè¡Œï¼ˆä¾‹å¦‚ -10 è¿”å›æœ€å10è¡Œï¼‰
                  - None: è¿”å›æ‰€æœ‰è¡Œ
            order_by: æ’åºåˆ—åï¼ˆå¯é€‰ï¼‰ï¼Œä¾‹å¦‚ 'datetime' æˆ– 'price'ï¼Œé»˜è®¤ä¸º'index'
            ascending: æ˜¯å¦å‡åºæ’åºï¼ˆé»˜è®¤Trueï¼‰
                      - True: å‡åºï¼ˆASCï¼‰
                      - False: é™åºï¼ˆDESCï¼‰
            columns: è¦è·å–çš„åˆ—ååˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
                    - None: è·å–æ‰€æœ‰åˆ—ï¼ˆé»˜è®¤ï¼‰
                    - ['col1', 'col2']: åªè·å–æŒ‡å®šçš„åˆ—
            
        Returns:
            pandas DataFrame æˆ– None
        """
        self._ensure_connection()
        
        try:
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            self.cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_name = %s
                );
            """, (table_name.split('.')[-1],))
            
            if not self.cursor.fetchone()['exists']:
                self.logger.error(f"è¡¨ {table_name} ä¸å­˜åœ¨")
                return None
            
            # è·å–è¡¨çš„æ‰€æœ‰åˆ—å
            table_columns = self._get_table_columns(table_name)
            # å¤„ç†è¦é€‰æ‹©çš„åˆ—
            select_clause = "*"
            if columns is not None:
                if not isinstance(columns, list):
                    self.logger.error(f"columns å‚æ•°å¿…é¡»æ˜¯åˆ—è¡¨ç±»å‹ï¼Œå½“å‰ç±»å‹: {type(columns)}")
                    return None
                
                if len(columns) == 0:
                    self.logger.error("columns å‚æ•°ä¸èƒ½æ˜¯ç©ºåˆ—è¡¨")
                    return None
                
                # æ¸…ç†å¹¶éªŒè¯åˆ—å
                cleaned_columns = []
                invalid_columns = []
                columns = ['index']+columns
                for col in columns:
                    col_clean = self._clean_column_name(col)
                    if col_clean in table_columns:
                        cleaned_columns.append(f'"{col_clean}"')
                    else:
                        invalid_columns.append(col)
                
                if invalid_columns:
                    self.logger.error(f"ä»¥ä¸‹åˆ—ä¸å­˜åœ¨äºè¡¨ä¸­: {invalid_columns}")
                    self.logger.info(f"å¯ç”¨çš„åˆ—: {table_columns}")
                    return None
                
                select_clause = ", ".join(cleaned_columns)
                self.logger.info(f"é€‰æ‹©åˆ—: {columns} (å…± {len(columns)} åˆ—)")
            
            # å¦‚æœæŒ‡å®šäº†æ’åºåˆ—ï¼ŒéªŒè¯åˆ—æ˜¯å¦å­˜åœ¨
            if order_by:
                # æ¸…ç†åˆ—åï¼ˆå¤„ç†ç‰¹æ®Šå­—ç¬¦ï¼‰
                order_by_clean = self._clean_column_name(order_by)
                
                if order_by_clean not in table_columns:
                    self.logger.error(f"æ’åºåˆ— '{order_by}' (æ¸…ç†å: '{order_by_clean}') ä¸å­˜åœ¨äºè¡¨ä¸­")
                    self.logger.info(f"å¯ç”¨çš„åˆ—: {table_columns}")
                    return None
                
                # å¦‚æœæŒ‡å®šäº† columns ä¸”æ’åºåˆ—ä¸åœ¨å…¶ä¸­ï¼Œéœ€è¦ä¸´æ—¶åŒ…å«æ’åºåˆ—
                if columns is not None:
                    columns_clean = [self._clean_column_name(col) for col in columns]
                    if order_by_clean not in columns_clean:
                        self.logger.warning(f"æ’åºåˆ— '{order_by_clean}' ä¸åœ¨é€‰æ‹©çš„åˆ—ä¸­ï¼Œå°†ä¸´æ—¶åŒ…å«ç”¨äºæ’åº")
                        # æ³¨æ„ï¼šè¿™é‡Œä¸ä¿®æ”¹ select_clauseï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥åœ¨ ORDER BY ä¸­ä½¿ç”¨ä¸åœ¨ SELECT ä¸­çš„åˆ—
            
            # æ„å»ºORDER BYå­å¥
            order_clause = ""
            if order_by:
                order_by_clean = self._clean_column_name(order_by)
                order_direction = "ASC" if ascending else "DESC"
                order_clause = f' ORDER BY "{order_by_clean}" {order_direction}'
                self.logger.info(f"æŒ‰åˆ— '{order_by_clean}' {'å‡åº' if ascending else 'é™åº'}æ’åº")
            
            # æ„å»ºæŸ¥è¯¢
            if limit is not None and limit < 0:
                # è´Ÿæ•°ï¼šè·å–æœ€å N è¡Œ
                # å…ˆè·å–æ€»è¡Œæ•°
                self.cursor.execute(f"SELECT COUNT(*) as total FROM {table_name}")
                total_rows = self.cursor.fetchone()['total']
                
                # è®¡ç®— OFFSET
                offset = max(0, total_rows + limit)  # limitæ˜¯è´Ÿæ•°ï¼Œæ‰€ä»¥ç›¸å½“äº total_rows - abs(limit)
                actual_limit = min(abs(limit), total_rows)
                
                query = f"SELECT {select_clause} FROM {table_name}{order_clause} OFFSET {offset} LIMIT {actual_limit}"
                self.logger.info(f"è·å–æœ€å {abs(limit)} è¡Œæ•°æ®ï¼ˆæ€»è¡Œæ•°: {total_rows}ï¼‰")
            elif limit is not None and limit > 0:
                # æ­£æ•°ï¼šè·å–å‰ N è¡Œ
                query = f"SELECT {select_clause} FROM {table_name}{order_clause} LIMIT {limit}"
            else:
                # None æˆ– 0ï¼šè·å–æ‰€æœ‰è¡Œ
                query = f"SELECT {select_clause} FROM {table_name}{order_clause}"
            
            # æ‰§è¡ŒæŸ¥è¯¢
            self.cursor.execute(query)
            results = self.cursor.fetchall()
            
            if not results:
                self.logger.warning(f"è¡¨ {table_name} ä¸ºç©º")
                return pd.DataFrame()
            
            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(results)
            
            self.logger.info(f"æˆåŠŸä»è¡¨ {table_name} åŠ è½½æ•°æ®ï¼Œå½¢çŠ¶: {df.shape}")
            return df
            
        except Exception as e:
            import traceback
            self.logger.error(f"åŠ è½½è¡¨ {table_name} å¤±è´¥: {str(e)}")
            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return None
    
    def list_tables(self, schema: str = 'public', verbose: bool = False, 
                    pattern: str = None, print_table: bool = False) -> List[Dict[str, Any]]:
        """
        åˆ—å‡ºæ•°æ®åº“ä¸­æ‰€æœ‰è¡¨
        
        Args:
            schema: æ¨¡å¼åï¼ˆé»˜è®¤ä¸ºpublicï¼‰
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆè¡Œæ•°ã€å¤§å°ç­‰ï¼‰
            pattern: è¡¨åè¿‡æ»¤æ¨¡å¼ï¼ˆæ”¯æŒSQL LIKEè¯­æ³•ï¼Œå¦‚ 'stock%'ï¼‰
            print_table: æ˜¯å¦ä»¥ç¾è§‚çš„è¡¨æ ¼å½¢å¼æ‰“å°
            
        Returns:
            è¡¨ä¿¡æ¯åˆ—è¡¨ï¼ŒåŒ…å«è¡¨åã€è¡Œæ•°ã€å¤§å°ç­‰ä¿¡æ¯
        """
        self._ensure_connection()
        
        try:
            if verbose:
                # è·å–è¯¦ç»†ä¿¡æ¯
                query = """
                    SELECT 
                        t.table_name,
                        pg_size_pretty(pg_total_relation_size(quote_ident(t.table_name)::regclass)) as size,
                        (SELECT COUNT(*) FROM information_schema.columns 
                         WHERE table_name = t.table_name AND table_schema = t.table_schema) as column_count
                    FROM information_schema.tables t
                    WHERE t.table_schema = %s
                """
                
                params = [schema]
                if pattern:
                    query += " AND t.table_name LIKE %s"
                    params.append(pattern)
                
                query += " ORDER BY t.table_name"
                
                self.cursor.execute(query, params)
                tables = []
                
                for row in self.cursor.fetchall():
                    table_name = row['table_name']
                    # è·å–è¡Œæ•°
                    self.cursor.execute(f'SELECT COUNT(*) as row_count FROM "{table_name}"')
                    row_count = self.cursor.fetchone()['row_count']
                    
                    tables.append({
                        'table_name': table_name,
                        'row_count': row_count,
                        'column_count': row['column_count'],
                        'size': row['size']
                    })
                
                if print_table:
                    self._print_tables_info(tables)
                
                self.logger.info(f"æ‰¾åˆ° {len(tables)} ä¸ªè¡¨")
                return tables
            else:
                # ç®€å•æ¨¡å¼ï¼šåªè¿”å›è¡¨å
                query = """
                    SELECT table_name 
                    FROM information_schema.tables 
                    WHERE table_schema = %s
                """
                
                params = [schema]
                if pattern:
                    query += " AND table_name LIKE %s"
                    params.append(pattern)
                
                query += " ORDER BY table_name"
                
                self.cursor.execute(query, params)
                tables = [{'table_name': row['table_name']} for row in self.cursor.fetchall()]
                
                if print_table:
                    print(f"\n[æ•°æ®åº“è¡¨åˆ—è¡¨] (å…± {len(tables)} ä¸ª):")
                    print("-" * 40)
                    for i, table in enumerate(tables, 1):
                        print(f"  {i}. {table['table_name']}")
                    print("-" * 40)
                
                self.logger.info(f"æ‰¾åˆ° {len(tables)} ä¸ªè¡¨")
                return tables
            
        except Exception as e:
            self.logger.error(f"è·å–è¡¨åˆ—è¡¨å¤±è´¥: {str(e)}")
            return []
    
    def _print_tables_info(self, tables: List[Dict[str, Any]]):
        """æ‰“å°è¡¨ä¿¡æ¯çš„ç¾è§‚æ ¼å¼"""
        if not tables:
            print("\n[è¡¨åˆ—è¡¨] æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¡¨")
            return
        
        print(f"\n{'='*80}")
        print(f"[æ•°æ®åº“è¡¨åˆ—è¡¨] (å…± {len(tables)} ä¸ªè¡¨)")
        print(f"{'='*80}")
        print(f"{'åºå·':<6} {'è¡¨å':<30} {'è¡Œæ•°':<12} {'åˆ—æ•°':<8} {'å¤§å°':<10}")
        print("-" * 80)
        
        for i, table in enumerate(tables, 1):
            print(f"{i:<6} {table['table_name']:<30} {table['row_count']:>10,}  "
                  f"{table['column_count']:>6}  {table['size']:>10}")
        
        print("=" * 80)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_rows = sum(t['row_count'] for t in tables)
        print(f"[ç»Ÿè®¡] æ€»è¡Œæ•°: {total_rows:,}")
        print("=" * 80 + "\n")
    
    def get_table_info(self, table_name: str, print_info: bool = False) -> Dict[str, Any]:
        """
        è·å–è¡¨çš„è¯¦ç»†ä¿¡æ¯
        
        Args:
            table_name: è¡¨å
            print_info: æ˜¯å¦ä»¥ç¾è§‚æ ¼å¼æ‰“å°ä¿¡æ¯
            
        Returns:
            è¡¨ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«åˆ—ä¿¡æ¯ã€ç´¢å¼•ã€çº¦æŸã€å¤§å°ç­‰
        """
        self._ensure_connection()
        
        try:
            table_name_only = table_name.split(".")[-1]
            
            # 1. è·å–åˆ—ä¿¡æ¯ï¼ˆåŒ…å«é»˜è®¤å€¼å’Œçº¦æŸï¼‰
            self.cursor.execute("""
                SELECT 
                    column_name, 
                    data_type, 
                    is_nullable,
                    column_default,
                    character_maximum_length
                FROM information_schema.columns 
                WHERE table_name = %s
                ORDER BY ordinal_position
            """, (table_name_only,))
            
            columns = self.cursor.fetchall()
            
            # 2. è·å–è¡Œæ•°
            self.cursor.execute(f'SELECT COUNT(*) as row_count FROM "{table_name_only}"')
            row_count = self.cursor.fetchone()['row_count']
            
            # 3. è·å–è¡¨å¤§å°
            self.cursor.execute("""
                SELECT pg_size_pretty(pg_total_relation_size(%s::regclass)) as size
            """, (table_name_only,))
            size = self.cursor.fetchone()['size']
            
            # 4. è·å–ç´¢å¼•ä¿¡æ¯
            self.cursor.execute("""
                SELECT
                    indexname as index_name,
                    indexdef as index_definition
                FROM pg_indexes
                WHERE tablename = %s
            """, (table_name_only,))
            
            indexes = self.cursor.fetchall()
            
            # 5. è·å–ä¸»é”®ä¿¡æ¯
            self.cursor.execute("""
                SELECT a.attname as column_name
                FROM pg_index i
                JOIN pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)
                WHERE i.indrelid = %s::regclass AND i.indisprimary
            """, (table_name_only,))
            
            primary_keys = [row['column_name'] for row in self.cursor.fetchall()]
            
            # 6. ç»„è£…ä¿¡æ¯
            info = {
                'table_name': table_name,
                'row_count': row_count,
                'column_count': len(columns),
                'size': size,
                'columns': columns,
                'indexes': indexes,
                'primary_keys': primary_keys
            }
            
            if print_info:
                self._print_table_info(info)
            
            self.logger.info(f"è¡¨ {table_name} ä¿¡æ¯: {len(columns)} åˆ—, {row_count} è¡Œ, {size}")
            return info
            
        except Exception as e:
            import traceback
            self.logger.error(f"è·å–è¡¨ä¿¡æ¯å¤±è´¥: {str(e)}")
            self.logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return {}
    
    def _print_table_info(self, info: Dict[str, Any]):
        """ä»¥ç¾è§‚æ ¼å¼æ‰“å°è¡¨ä¿¡æ¯"""
        print(f"\n{'='*80}")
        print(f"[è¡¨ä¿¡æ¯] {info['table_name']}")
        print(f"{'='*80}")
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"\n[åŸºæœ¬ç»Ÿè®¡]")
        print(f"  - æ€»è¡Œæ•°: {info['row_count']:,}")
        print(f"  - æ€»åˆ—æ•°: {info['column_count']}")
        print(f"  - è¡¨å¤§å°: {info['size']}")
        
        # ä¸»é”®ä¿¡æ¯
        if info['primary_keys']:
            print(f"\n[ä¸»é”®]")
            for pk in info['primary_keys']:
                print(f"  - {pk}")
        
        # åˆ—ä¿¡æ¯
        print(f"\n[åˆ—è¯¦æƒ…]")
        print(f"{'åºå·':<6} {'åˆ—å':<25} {'ç±»å‹':<20} {'å¯ç©º':<8} {'é»˜è®¤å€¼':<15}")
        print("-" * 80)
        
        for i, col in enumerate(info['columns'], 1):
            col_name = col['column_name']
            data_type = col['data_type']
            if col.get('character_maximum_length'):
                data_type += f"({col['character_maximum_length']})"
            
            nullable = "Y" if col['is_nullable'] == 'YES' else "N"
            default = str(col['column_default'])[:15] if col['column_default'] else "-"
            
            # æ ‡è®°ä¸»é”®
            if col_name in info['primary_keys']:
                col_name += " [PK]"
            
            print(f"{i:<6} {col_name:<25} {data_type:<20} {nullable:<8} {default:<15}")
        
        # ç´¢å¼•ä¿¡æ¯
        if info['indexes']:
            print(f"\n[ç´¢å¼•] (å…± {len(info['indexes'])} ä¸ª)")
            for i, idx in enumerate(info['indexes'], 1):
                print(f"  {i}. {idx['index_name']}")
                # ç®€åŒ–ç´¢å¼•å®šä¹‰æ˜¾ç¤º
                idx_def = idx['index_definition']
                if len(idx_def) > 70:
                    idx_def = idx_def[:70] + "..."
                print(f"     {idx_def}")
        
        print("=" * 80 + "\n")
    
    @staticmethod
    def help():
        """
        æ˜¾ç¤º EasyManager çš„åŠŸèƒ½å¸®åŠ©ä¿¡æ¯
        """
        help_text = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                      EasyManager ä½¿ç”¨å¸®åŠ© (v2.2)                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“š æ ¸å¿ƒåŠŸèƒ½ï¼š

  1. create_table(table_name, dataframe, overwrite=False)
     â””â”€ åˆ›å»ºè¡¨å¹¶å¯¼å…¥æ•°æ®
     â””â”€ ç¤ºä¾‹: em.create_table('my_table', df, overwrite=True)

  2. add_columns(table_name, dataframe, merge_on_index=True)  
     â””â”€ åœ¨å·²å­˜åœ¨çš„è¡¨ä¸­æ·»åŠ æ–°åˆ—ï¼ˆè‡ªåŠ¨å±è”½å·²å­˜åœ¨çš„åˆ—ï¼‰
     â””â”€ ç¤ºä¾‹: em.add_columns('my_table', df_new_cols)

  3. insert_data(table_name, dataframe, mode='skip')  
     â””â”€ æ’å…¥æ•°æ®ï¼Œæ”¯æŒä¸‰ç§æ¨¡å¼ï¼š
        â€¢ skip   - å¿½ç•¥é‡å¤ç´¢å¼•ï¼ˆé»˜è®¤ï¼‰
        â€¢ update - è¦†ç›–é‡å¤ç´¢å¼•çš„æ•°æ®
        â€¢ append - ç›´æ¥è¿½åŠ ï¼Œä¸æ£€æŸ¥é‡å¤
     â””â”€ ç¤ºä¾‹: em.insert_data('my_table', df, mode='skip')

  4. load_table(table_name, limit=None, order_by='index', ascending=True, columns=None)  â­â­ å…¨åŠŸèƒ½ç‰ˆ
     â””â”€ ä»æ•°æ®åº“å¯¼å…¥è¡¨åˆ° Python
     â””â”€ limitå‚æ•°: æ­£æ•°=å‰Nè¡Œ, è´Ÿæ•°=åNè¡Œ, None=å…¨éƒ¨
     â””â”€ order_byå‚æ•°: æŒ‡å®šæ’åºåˆ—åï¼ˆé»˜è®¤'index'ï¼‰
     â””â”€ ascendingå‚æ•°: True=å‡åº, False=é™åº
     â””â”€ columnså‚æ•°: æŒ‡å®šè¦è·å–çš„åˆ—ï¼ˆåˆ—è¡¨ï¼‰ï¼ŒNone=å…¨éƒ¨åˆ—
     â””â”€ ç¤ºä¾‹: 
        â€¢ df = em.load_table('my_table', limit=100)                              # å‰100è¡Œ
        â€¢ df = em.load_table('my_table', limit=-50)                              # æœ€å50è¡Œ
        â€¢ df = em.load_table('my_table', order_by='datetime')                    # æŒ‰æ—¥æœŸå‡åº
        â€¢ df = em.load_table('my_table', order_by='price', ascending=False)      # æŒ‰ä»·æ ¼é™åº
        â€¢ df = em.load_table('my_table', columns=['datetime', 'price'])          # åªè·å–æŒ‡å®šåˆ—
        â€¢ df = em.load_table('my_table', limit=10, order_by='datetime', 
                            ascending=False, columns=['datetime', 'company', 'price'])  # ç»„åˆä½¿ç”¨

  5. drop_table(table_name)
     â””â”€ åˆ é™¤è¡¨
     â””â”€ ç¤ºä¾‹: em.drop_table('my_table')

  6. list_tables(schema='public', verbose=False, pattern=None)  â­ å‡çº§ç‰ˆ
     â””â”€ åˆ—å‡ºæ‰€æœ‰è¡¨ï¼ˆæ”¯æŒè¯¦ç»†æ¨¡å¼å’Œè¿‡æ»¤ï¼‰
     â””â”€ ç¤ºä¾‹: em.list_tables(pattern='stock%', verbose=True, print_table=True)

  7. get_table_info(table_name, print_info=False)  â­ å‡çº§ç‰ˆ
     â””â”€ è·å–è¡¨è¯¦ç»†ä¿¡æ¯ï¼ˆåˆ—ã€è¡Œæ•°ã€å¤§å°ã€ä¸»é”®ã€ç´¢å¼•ï¼‰
     â””â”€ ç¤ºä¾‹: em.get_table_info('my_table', print_info=True)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ¯ ä¸‰ç§æ’å…¥æ¨¡å¼è¯¦è§£ï¼š

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  æ¨¡å¼   â”‚ æ£€æŸ¥æ–¹å¼ â”‚ éœ€è¦ç´¢å¼• â”‚   æ€§èƒ½   â”‚     é€‚ç”¨åœºæ™¯      â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ skip    â”‚ åŸºäºç´¢å¼• â”‚   âœ…     â”‚   ä¸­ç­‰   â”‚ å¢é‡æ›´æ–°ï¼Œé¿å…é‡å¤â”‚
  â”‚ update  â”‚ åŸºäºç´¢å¼• â”‚   âœ…     â”‚   è¾ƒæ…¢   â”‚ æ•°æ®ä¿®æ­£ï¼ŒUPSERT  â”‚
  â”‚ append  â”‚ ä¸æ£€æŸ¥   â”‚   âŒ     â”‚   æœ€å¿«   â”‚ å¿«é€Ÿæ‰¹é‡å¯¼å…¥      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ’¡ å¿«é€Ÿç¤ºä¾‹ï¼š

  # 1. è¿æ¥æ•°æ®åº“
  from easy_manager import EasyManager
  import pandas as pd
  
  with EasyManager() as em:
      
      # 2. åˆ›å»ºè¡¨
      df = pd.read_csv('data.csv', index_col=0)
      em.create_table('stocks', df)
      
      # 3. æ·»åŠ æ–°åˆ—
      df_new = pd.read_csv('new_factors.csv', index_col=0)
      em.add_columns('stocks', df_new)
      
      # 4. æ’å…¥æ•°æ®ï¼ˆä¸‰ç§æ¨¡å¼ï¼‰
      em.insert_data('stocks', df, mode='skip')    # å¿½ç•¥é‡å¤ç´¢å¼•
      em.insert_data('stocks', df, mode='update')  # è¦†ç›–é‡å¤æ•°æ®
      em.insert_data('stocks', df, mode='append')  # ç›´æ¥è¿½åŠ 
      
      # 5. å¯¼å…¥è¡¨ï¼ˆæ”¯æŒæ’åºã€é™åˆ¶å’Œåˆ—é€‰æ‹©ï¼‰
      df_loaded = em.load_table('stocks')                           # å…¨éƒ¨æ•°æ®
      df_top10 = em.load_table('stocks', limit=10)                  # å‰10è¡Œ
      df_last10 = em.load_table('stocks', limit=-10)                # æœ€å10è¡Œ
      df_sorted = em.load_table('stocks', order_by='datetime')      # æŒ‰æ—¥æœŸæ’åº
      df_cols = em.load_table('stocks', columns=['datetime', 'price', 'volume'])  # åªè·å–ç‰¹å®šåˆ—
      df_latest = em.load_table('stocks', limit=10, order_by='datetime', 
                                ascending=False, columns=['datetime', 'price'])  # ç»„åˆä½¿ç”¨
      
      # 6. æŸ¥è¯¢è¡¨ä¿¡æ¯
      tables = em.list_tables()
      info = em.get_table_info('stocks')
      
      # 7. åˆ é™¤è¡¨
      em.drop_table('stocks')

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âš ï¸  æ³¨æ„äº‹é¡¹ï¼š

  â€¢ skip å’Œ update æ¨¡å¼éœ€è¦ DataFrame æœ‰ç´¢å¼•åˆ—
  â€¢ åˆ—åä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼ˆ., -, ç©ºæ ¼ï¼‰ä¼šè‡ªåŠ¨è½¬æ¢ä¸º _
  â€¢ æ‰€æœ‰æ“ä½œè®°å½•åœ¨ datadeal.log æ–‡ä»¶ä¸­
  â€¢ columns å‚æ•°å¯ä»¥å‡å°‘æ•°æ®ä¼ è¾“é‡ï¼Œæé«˜å¤§è¡¨æŸ¥è¯¢æ€§èƒ½
  â€¢ å¦‚æœæ’åºåˆ—ä¸åœ¨ columns ä¸­ï¼Œä»å¯æ­£å¸¸æ’åºï¼ˆä½†æ’åºåˆ—ä¸ä¼šå‡ºç°åœ¨ç»“æœä¸­ï¼‰

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“– æ›´å¤šä¿¡æ¯ï¼š

  â€¢ å®Œæ•´æ‰‹å†Œï¼šEasyManagerå®Œæ•´ä½¿ç”¨æ‰‹å†Œ.md
  â€¢ æµ‹è¯•ç¤ºä¾‹ï¼štest_new_features.py
  â€¢ åœ¨çº¿å¸®åŠ©ï¼šEasyManager.help()

â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """
        print(help_text)
    
    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.cursor:
            self.cursor.close()
        if self.conn:
            self.conn.close()
        self.logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()


class LongManager(EasyManager):
    """
    é•¿æ ¼å¼æ•°æ®ç®¡ç†å™¨ï¼ˆPanel Data Managerï¼‰
    
    ä¸“é—¨ç”¨äºå¤„ç†é•¿æ ¼å¼é¢æ¿æ•°æ®ï¼Œç‰¹ç‚¹ï¼š
    1. æ•°æ®æ ¼å¼ï¼šæ¯è¡Œæ˜¯ä¸€ä¸ªå…¬å¸åœ¨æŸæ—¶é—´ç‚¹çš„è§‚æµ‹
    2. å¤åˆé”®ï¼šä½¿ç”¨ï¼ˆæ—¶é—´ï¼Œå…¬å¸ï¼‰ä½œä¸ºå”¯ä¸€æ ‡è¯†
    3. åˆ—é¡ºåºï¼šæ—¶é—´åˆ—åœ¨ç¬¬ä¸€åˆ—ï¼Œå…¬å¸åˆ—åœ¨ç¬¬äºŒåˆ—
    4. ç´¢å¼•ï¼šä½¿ç”¨è‡ªå¢åºåˆ—ï¼Œè€Œä¸æ˜¯æ—¶é—´ç´¢å¼•
    
    é€‚ç”¨åœºæ™¯ï¼š
    - å¤šå…¬å¸å¤šæ—¶é—´ç‚¹çš„å› å­æ•°æ®
    - Panel Data åˆ†æ
    - æ—¶é—´åºåˆ—æ¨ªæˆªé¢æ•°æ®
    """
    
    def __init__(self, 
                 database: str = 'test_data_base',
                 user: str = 'postgres',
                 password: str = 'cbw88982449',
                 host: str = 'localhost',
                 port: int = 5432,
                 time_col: str = 'datetime',
                 entity_col: str = 'company'):
        """
        åˆå§‹åŒ–é•¿æ ¼å¼æ•°æ®ç®¡ç†å™¨
        
        Args:
            database: æ•°æ®åº“å
            user: ç”¨æˆ·å
            password: å¯†ç 
            host: ä¸»æœºåœ°å€
            port: ç«¯å£
            time_col: æ—¶é—´åˆ—åï¼ˆé»˜è®¤ï¼š'datetime'ï¼‰
            entity_col: å®ä½“åˆ—åï¼ˆé»˜è®¤ï¼š'company'ï¼‰
        """
        super().__init__(database, user, password, host, port)
        self.time_col = time_col
        self.entity_col = entity_col
        self.logger.info(f"LongManager åˆå§‹åŒ–å®Œæˆï¼Œå¤åˆé”®ï¼š({time_col}, {entity_col})")
    
    @function_timer
    def create_table(self, table_name: str, dataframe: pd.DataFrame, 
                     overwrite: bool = False) -> bool:
        """
        åˆ›å»ºé•¿æ ¼å¼æ•°æ®è¡¨
        
        ç‰¹ç‚¹ï¼š
        1. ç¡®ä¿æ—¶é—´åˆ—åœ¨ç¬¬ä¸€åˆ—ï¼Œå…¬å¸åˆ—åœ¨ç¬¬äºŒåˆ—
        2. ä¸ä½¿ç”¨è¿™ä¸¤åˆ—ä½œä¸ºç´¢å¼•ï¼Œä½¿ç”¨è‡ªå¢åºåˆ—
        3. è‡ªåŠ¨æ£€æŸ¥å’Œè°ƒæ•´åˆ—é¡ºåº
        
        Args:
            table_name: è¡¨å
            dataframe: DataFrameï¼ˆå¿…é¡»åŒ…å«æ—¶é—´åˆ—å’Œå®ä½“åˆ—ï¼‰
            overwrite: æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„è¡¨
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        self._ensure_connection()
        
        try:
            # æ£€æŸ¥å¿…éœ€çš„åˆ—
            if self.time_col not in dataframe.columns:
                self.logger.error(f"DataFrame ç¼ºå°‘æ—¶é—´åˆ—: {self.time_col}")
                return False
            
            if self.entity_col not in dataframe.columns:
                self.logger.error(f"DataFrame ç¼ºå°‘å®ä½“åˆ—: {self.entity_col}")
                return False
            
            # é‡ç½®ç´¢å¼•ï¼ˆå¦‚æœæœ‰ï¼‰
            df = dataframe.copy()
            if df.index.name is not None or not isinstance(df.index, pd.RangeIndex):
                df = df.reset_index(drop=True)
            
            # è°ƒæ•´åˆ—é¡ºåºï¼šæ—¶é—´åˆ—ç¬¬ä¸€ï¼Œå…¬å¸åˆ—ç¬¬äºŒï¼Œå…¶ä»–åˆ—ä¿æŒé¡ºåº
            other_cols = [col for col in df.columns 
                         if col not in [self.time_col, self.entity_col]]
            df = df[[self.time_col, self.entity_col] + other_cols]
            
            # ç¡®ä¿æ—¶é—´åˆ—æ˜¯ datetime ç±»å‹
            if not pd.api.types.is_datetime64_any_dtype(df[self.time_col]):
                self.logger.warning(f"æ—¶é—´åˆ— {self.time_col} ä¸æ˜¯ datetime ç±»å‹ï¼Œæ­£åœ¨è½¬æ¢...")
                df[self.time_col] = pd.to_datetime(df[self.time_col])
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„ï¼ˆæ—¶é—´ï¼Œå…¬å¸ï¼‰ç»„åˆ
            duplicates = df.duplicated(subset=[self.time_col, self.entity_col], keep=False)
            if duplicates.any():
                dup_count = duplicates.sum()
                self.logger.warning(f"å‘ç° {dup_count} ä¸ªé‡å¤çš„ ({self.time_col}, {self.entity_col}) ç»„åˆ")
                self.logger.warning("å°†ä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„è®°å½•")
                df = df.drop_duplicates(subset=[self.time_col, self.entity_col], keep='first')
            
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            self.cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_name = %s
                );
            """, (table_name.split('.')[-1],))
            
            table_exists = self.cursor.fetchone()['exists']
            
            if table_exists and not overwrite:
                self.logger.warning(f"è¡¨ {table_name} å·²å­˜åœ¨ï¼Œä½¿ç”¨ overwrite=True æ¥è¦†ç›–")
                return False
            
            if table_exists and overwrite:
                self.logger.info(f"åˆ é™¤å·²å­˜åœ¨çš„è¡¨ {table_name}")
                self.cursor.execute(f'DROP TABLE IF EXISTS "{table_name}" CASCADE')
                self.conn.commit()
            
            # æ¸…ç†åˆ—å
            df.columns = [self._clean_column_name(col) for col in df.columns]
            
            # åˆ›å»ºè¡¨ç»“æ„ï¼ˆæ·»åŠ è‡ªå¢ä¸»é”®ï¼‰
            columns_def = ['id SERIAL PRIMARY KEY']
            
            for column in df.columns:
                col_type = self._infer_column_type(df[column])
                nullable = "NULL" if df[column].isnull().any() else "NOT NULL"
                columns_def.append(f'"{column}" {col_type} {nullable}')
            
            create_table_sql = f'CREATE TABLE "{table_name}" ({", ".join(columns_def)})'
            
            self.logger.info(f"åˆ›å»ºè¡¨ {table_name}ï¼Œåˆ—æ•°: {len(df.columns)}")
            self.cursor.execute(create_table_sql)
            self.conn.commit()
            
            # åœ¨ï¼ˆæ—¶é—´ï¼Œå…¬å¸ï¼‰åˆ—ä¸Šåˆ›å»ºå¤åˆç´¢å¼•ï¼Œæé«˜æŸ¥è¯¢æ€§èƒ½
            index_name = f"{table_name}_{self.time_col}_{self.entity_col}_idx"
            create_index_sql = f'''
                CREATE INDEX "{index_name}" 
                ON "{table_name}" ("{self.time_col}", "{self.entity_col}")
            '''
            self.cursor.execute(create_index_sql)
            self.conn.commit()
            self.logger.info(f"å·²åˆ›å»ºå¤åˆç´¢å¼•: {index_name}")
            
            # æ’å…¥æ•°æ®
            self._insert_dataframe(table_name, df)
            
            self.logger.info(f"æˆåŠŸåˆ›å»ºè¡¨ {table_name}ï¼Œæ’å…¥ {len(df)} è¡Œæ•°æ®")
            return True
            
        except Exception as e:
            self.conn.rollback()
            import traceback
            self.logger.error(f"åˆ›å»ºè¡¨ {table_name} å¤±è´¥: {str(e)}")
            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return False
    
    @function_timer
    def insert_data(self, table_name: str, dataframe: pd.DataFrame, 
                    mode: str = 'skip') -> bool:
        """
        æ’å…¥é•¿æ ¼å¼æ•°æ®
        
        åŸºäºï¼ˆæ—¶é—´ï¼Œå…¬å¸ï¼‰å¤åˆé”®åˆ¤æ–­é‡å¤ï¼š
        - skip: å¿½ç•¥é‡å¤çš„ï¼ˆæ—¶é—´ï¼Œå…¬å¸ï¼‰ç»„åˆ
        - update: æ›´æ–°é‡å¤çš„ï¼ˆæ—¶é—´ï¼Œå…¬å¸ï¼‰ç»„åˆ
        - append: ç›´æ¥è¿½åŠ ï¼Œä¸æ£€æŸ¥é‡å¤
        
        Args:
            table_name: è¡¨å
            dataframe: DataFrame
            mode: 'skip', 'update', 'append'
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        self._ensure_connection()
        
        if mode not in ['skip', 'update', 'append']:
            self.logger.error(f"ä¸æ”¯æŒçš„æ¨¡å¼: {mode}")
            return False
        
        try:
            # æ£€æŸ¥å¿…éœ€çš„åˆ—
            if self.time_col not in dataframe.columns:
                self.logger.error(f"DataFrame ç¼ºå°‘æ—¶é—´åˆ—: {self.time_col}")
                return False
            
            if self.entity_col not in dataframe.columns:
                self.logger.error(f"DataFrame ç¼ºå°‘å®ä½“åˆ—: {self.entity_col}")
                return False
            
            # å‡†å¤‡æ•°æ®
            df = dataframe.copy()
            if df.index.name is not None or not isinstance(df.index, pd.RangeIndex):
                df = df.reset_index(drop=True)
            
            # ç¡®ä¿åˆ—é¡ºåº
            other_cols = [col for col in df.columns 
                         if col not in [self.time_col, self.entity_col]]
            df = df[[self.time_col, self.entity_col] + other_cols]
            
            # ç¡®ä¿æ—¶é—´åˆ—æ˜¯ datetime ç±»å‹
            if not pd.api.types.is_datetime64_any_dtype(df[self.time_col]):
                df[self.time_col] = pd.to_datetime(df[self.time_col])
            
            # æ¸…ç†åˆ—å
            df.columns = [self._clean_column_name(col) for col in df.columns]
            
            # append æ¨¡å¼ï¼šç›´æ¥æ’å…¥
            if mode == 'append':
                self._insert_dataframe(table_name, df)
                self.logger.info(f"append æ¨¡å¼ï¼šæ’å…¥ {len(df)} è¡Œæ•°æ®")
                return True
            
            # skip å’Œ update æ¨¡å¼ï¼šéœ€è¦æ£€æŸ¥é‡å¤
            # åŠ è½½ç°æœ‰æ•°æ®çš„ï¼ˆæ—¶é—´ï¼Œå…¬å¸ï¼‰ç»„åˆ
            query = f'''
                SELECT "{self.time_col}", "{self.entity_col}"
                FROM "{table_name}"
            '''
            existing_df = pd.read_sql(query, self.conn)
            existing_df[self.time_col] = pd.to_datetime(existing_df[self.time_col])
            
            # åˆ›å»ºå¤åˆé”®
            existing_keys = set(
                zip(existing_df[self.time_col], existing_df[self.entity_col])
            )
            df_keys = list(zip(df[self.time_col], df[self.entity_col]))
            
            if mode == 'skip':
                # skip æ¨¡å¼ï¼šåªæ’å…¥æ–°çš„ï¼ˆæ—¶é—´ï¼Œå…¬å¸ï¼‰ç»„åˆ
                mask = [key not in existing_keys for key in df_keys]
                df_to_insert = df[mask].copy()
                
                if len(df_to_insert) == 0:
                    self.logger.info("skip æ¨¡å¼ï¼šæ‰€æœ‰æ•°æ®éƒ½å·²å­˜åœ¨ï¼Œæ— éœ€æ’å…¥")
                    return True
                
                self._insert_dataframe(table_name, df_to_insert)
                skipped = len(df) - len(df_to_insert)
                self.logger.info(
                    f"skip æ¨¡å¼ï¼šæ’å…¥ {len(df_to_insert)} è¡Œæ–°æ•°æ®ï¼Œ"
                    f"è·³è¿‡ {skipped} è¡Œé‡å¤æ•°æ®"
                )
                return True
            
            elif mode == 'update':
                # update æ¨¡å¼ï¼šæ›´æ–°å·²å­˜åœ¨çš„ï¼Œæ’å…¥æ–°çš„
                mask_update = [key in existing_keys for key in df_keys]
                mask_insert = [key not in existing_keys for key in df_keys]
                
                df_to_update = df[mask_update].copy()
                df_to_insert = df[mask_insert].copy()
                
                # æ’å…¥æ–°æ•°æ®
                if len(df_to_insert) > 0:
                    self._insert_dataframe(table_name, df_to_insert)
                    self.logger.info(f"æ’å…¥ {len(df_to_insert)} è¡Œæ–°æ•°æ®")
                
                # æ›´æ–°å·²å­˜åœ¨çš„æ•°æ®
                if len(df_to_update) > 0:
                    update_count = 0
                    columns = [col for col in df_to_update.columns 
                              if col not in [self.time_col, self.entity_col]]
                    
                    for _, row in df_to_update.iterrows():
                        set_clause = ', '.join([f'"{col}" = %s' for col in columns])
                        update_sql = f'''
                            UPDATE "{table_name}"
                            SET {set_clause}
                            WHERE "{self.time_col}" = %s AND "{self.entity_col}" = %s
                        '''
                        
                        values = [None if pd.isna(row[col]) else row[col] 
                                 for col in columns]
                        values.extend([row[self.time_col], row[self.entity_col]])
                        
                        self.cursor.execute(update_sql, values)
                        if self.cursor.rowcount > 0:
                            update_count += 1
                    
                    self.conn.commit()
                    self.logger.info(f"æ›´æ–° {update_count} è¡Œæ•°æ®")
                
                return True
            
        except Exception as e:
            self.conn.rollback()
            import traceback
            self.logger.error(f"æ’å…¥æ•°æ®åˆ°è¡¨ {table_name} å¤±è´¥: {str(e)}")
            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            self.logger.error("è¯·ä¼˜å…ˆæ£€æŸ¥æ•°æ®æ ¼å¼é—®é¢˜,æ³¨:æ‰€æœ‰çš„æ—¶é—´æ ¼å¼éƒ½éœ€è¦pd.to_datetimeåæ–¹å¯å½•å…¥")
            return False
    
    @function_timer
    def add_columns(self, table_name: str, dataframe: pd.DataFrame, 
                    merge_on_keys: bool = True) -> bool:
        """
        å‘é•¿æ ¼å¼è¡¨æ·»åŠ æ–°åˆ—
        
        åŸºäºï¼ˆæ—¶é—´ï¼Œå…¬å¸ï¼‰å¤åˆé”®åˆå¹¶æ•°æ®
        
        Args:
            table_name: è¡¨å
            dataframe: åŒ…å«æ–°åˆ—çš„ DataFrame
            merge_on_keys: æ˜¯å¦åŸºäºï¼ˆæ—¶é—´ï¼Œå…¬å¸ï¼‰åˆå¹¶æ•°æ®
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        self._ensure_connection()
        
        try:
            # æ£€æŸ¥å¿…éœ€çš„åˆ—
            if self.time_col not in dataframe.columns:
                self.logger.error(f"DataFrame ç¼ºå°‘æ—¶é—´åˆ—: {self.time_col}")
                return False
            
            if self.entity_col not in dataframe.columns:
                self.logger.error(f"DataFrame ç¼ºå°‘å®ä½“åˆ—: {self.entity_col}")
                return False
            
            # å‡†å¤‡æ•°æ®
            df = dataframe.copy()
            if df.index.name is not None or not isinstance(df.index, pd.RangeIndex):
                df = df.reset_index(drop=True)
            
            # ç¡®ä¿æ—¶é—´åˆ—æ˜¯ datetime ç±»å‹
            if not pd.api.types.is_datetime64_any_dtype(df[self.time_col]):
                df[self.time_col] = pd.to_datetime(df[self.time_col])
            
            # æ¸…ç†åˆ—å
            df.columns = [self._clean_column_name(col) for col in df.columns]
            
            # è·å–ç°æœ‰åˆ—
            existing_columns = self._get_table_columns(table_name)
            
            # è¯†åˆ«æ–°åˆ—ï¼ˆæ’é™¤æ—¶é—´åˆ—å’Œå®ä½“åˆ—ï¼‰
            new_columns = [col for col in df.columns 
                          if col not in existing_columns 
                          and col not in [self.time_col, self.entity_col]]
            
            if not new_columns:
                self.logger.info("æ²¡æœ‰éœ€è¦æ·»åŠ çš„æ–°åˆ—")
                return True
            
            self.logger.info(f"å‡†å¤‡æ·»åŠ  {len(new_columns)} ä¸ªæ–°åˆ—: {new_columns}")
            
            # æ·»åŠ æ–°åˆ—åˆ°è¡¨ç»“æ„
            for column in new_columns:
                col_type = self._infer_column_type(df[column])
                alter_sql = f'ALTER TABLE "{table_name}" ADD COLUMN "{column}" {col_type}'
                self.cursor.execute(alter_sql)
                self.logger.info(f"æ·»åŠ åˆ—: {column} ({col_type})")
            
            self.conn.commit()
            
            # å¦‚æœéœ€è¦åˆå¹¶æ•°æ®
            if merge_on_keys and new_columns:
                self.logger.info("å¼€å§‹åŸºäºï¼ˆæ—¶é—´ï¼Œå…¬å¸ï¼‰é”®åˆå¹¶æ•°æ®...")
                
                # åŠ è½½ç°æœ‰æ•°æ®
                query = f'SELECT "{self.time_col}", "{self.entity_col}" FROM "{table_name}"'
                existing_df = pd.read_sql(query, self.conn)
                existing_df[self.time_col] = pd.to_datetime(existing_df[self.time_col])
                
                # åˆ›å»ºé”®é›†åˆ
                existing_keys = set(
                    zip(existing_df[self.time_col], existing_df[self.entity_col])
                )
                
                update_count = 0
                for _, row in df.iterrows():
                    key = (row[self.time_col], row[self.entity_col])
                    
                    if key in existing_keys:
                        set_clause = ', '.join([f'"{col}" = %s' for col in new_columns])
                        update_sql = f'''
                            UPDATE "{table_name}"
                            SET {set_clause}
                            WHERE "{self.time_col}" = %s AND "{self.entity_col}" = %s
                        '''
                        
                        values = [None if pd.isna(row[col]) else row[col] 
                                 for col in new_columns]
                        values.extend([row[self.time_col], row[self.entity_col]])
                        
                        self.cursor.execute(update_sql, values)
                        if self.cursor.rowcount > 0:
                            update_count += 1
                
                self.conn.commit()
                self.logger.info(f"æˆåŠŸæ›´æ–° {update_count} è¡Œçš„æ–°åˆ—æ•°æ®")
            
            return True
            
        except Exception as e:
            self.conn.rollback()
            import traceback
            self.logger.error(f"æ·»åŠ åˆ—åˆ°è¡¨ {table_name} å¤±è´¥: {str(e)}")
            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            self.logger.error("è¯·ä¼˜å…ˆæ£€æŸ¥æ•°æ®æ ¼å¼é—®é¢˜,æ³¨:æ‰€æœ‰çš„æ—¶é—´æ ¼å¼éƒ½éœ€è¦pd.to_datetimeåæ–¹å¯å½•å…¥")
            return False
    
    @staticmethod
    def help():
        """æ˜¾ç¤º LongManager çš„å¸®åŠ©ä¿¡æ¯"""
        help_text = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    LongManager ä½¿ç”¨å¸®åŠ© (v1.0)                             â•‘
â•‘                   é•¿æ ¼å¼ï¼ˆPanel Dataï¼‰æ•°æ®ç®¡ç†å™¨                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[æ ¸å¿ƒç‰¹ç‚¹]

  - ä¸“é—¨å¤„ç†é•¿æ ¼å¼é¢æ¿æ•°æ®ï¼ˆPanel Dataï¼‰
  - ä½¿ç”¨ï¼ˆæ—¶é—´ï¼Œå…¬å¸ï¼‰ä½œä¸ºå¤åˆé”®åˆ¤æ–­å”¯ä¸€æ€§
  - æ—¶é—´åˆ—åœ¨ç¬¬ä¸€åˆ—ï¼Œå…¬å¸åˆ—åœ¨ç¬¬äºŒåˆ—
  - ä½¿ç”¨è‡ªå¢åºåˆ—ä½œä¸ºä¸»é”®ï¼Œè€Œéæ—¶é—´ç´¢å¼•

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

[æ•°æ®æ ¼å¼]

  é•¿æ ¼å¼æ•°æ®çš„ç‰¹ç‚¹ï¼š
  - æ¯è¡Œæ˜¯ä¸€ä¸ªå…¬å¸åœ¨æŸæ—¶é—´ç‚¹çš„è§‚æµ‹
  - åŒä¸€æ—¶é—´æœ‰å¤šä¸ªå…¬å¸
  - åŒä¸€å…¬å¸æœ‰å¤šä¸ªæ—¶é—´ç‚¹

  ç¤ºä¾‹ï¼š
  +------------+---------+----------+----------+-----+
  |  datetime  | company | factor_A | factor_B | ... |
  +------------+---------+----------+----------+-----+
  | 2020-01-01 |  AAPL   |   25.3   |   0.15   | ... |
  | 2020-01-01 |  GOOGL  |   28.7   |   0.18   | ... |
  | 2020-01-02 |  AAPL   |   25.5   |   0.16   | ... |
  | 2020-01-02 |  GOOGL  |   28.9   |   0.19   | ... |
  +------------+---------+----------+----------+-----+

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

[ä¸»è¦åŠŸèƒ½]

  1. create_table(table_name, dataframe, overwrite=False)
     - åˆ›å»ºé•¿æ ¼å¼æ•°æ®è¡¨
     - è‡ªåŠ¨è°ƒæ•´åˆ—é¡ºåºï¼ˆæ—¶é—´åˆ—ç¬¬ä¸€ï¼Œå…¬å¸åˆ—ç¬¬äºŒï¼‰
     - è‡ªåŠ¨åˆ›å»ºå¤åˆç´¢å¼•æé«˜æŸ¥è¯¢æ€§èƒ½

  2. insert_data(table_name, dataframe, mode='skip')
     - åŸºäºï¼ˆæ—¶é—´ï¼Œå…¬å¸ï¼‰åˆ¤æ–­é‡å¤
     - skip: å¿½ç•¥é‡å¤é”®
     - update: æ›´æ–°é‡å¤é”®
     - append: ç›´æ¥è¿½åŠ 

  3. add_columns(table_name, dataframe, merge_on_keys=True)
     - æ·»åŠ æ–°å› å­åˆ—
     - åŸºäºï¼ˆæ—¶é—´ï¼Œå…¬å¸ï¼‰åˆå¹¶æ•°æ®

  4. ç»§æ‰¿ EasyManager çš„æ‰€æœ‰å…¶ä»–åŠŸèƒ½
     - load_table, drop_table, list_tables, get_table_info

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

[å¿«é€Ÿç¤ºä¾‹]

  from easy_manager import LongManager
  import pandas as pd

  # 1. åˆå§‹åŒ–ï¼ˆå¯è‡ªå®šä¹‰æ—¶é—´åˆ—å’Œå®ä½“åˆ—åï¼‰
  with LongManager(time_col='datetime', entity_col='company') as lm:
      
      # 2. è¯»å–é•¿æ ¼å¼æ•°æ®
      df = pd.read_csv('long_data/full_factors.csv')
      df['datetime'] = pd.to_datetime(df['datetime'])
      
      # 3. åˆ›å»ºè¡¨ï¼ˆè‡ªåŠ¨å¤„ç†åˆ—é¡ºåºå’Œç´¢å¼•ï¼‰
      lm.create_table('factor_panel', df)
      
      # 4. æ·»åŠ æ–°å› å­åˆ—
      df_new = pd.read_csv('long_data/new_factors.csv')
      df_new['datetime'] = pd.to_datetime(df_new['datetime'])
      lm.add_columns('factor_panel', df_new)
      
      # 5. æ’å…¥å¢é‡æ•°æ®ï¼ˆåŸºäºæ—¶é—´-å…¬å¸é”®å»é‡ï¼‰
      df_new_data = pd.read_csv('long_data/incremental.csv')
      df_new_data['datetime'] = pd.to_datetime(df_new_data['datetime'])
      lm.insert_data('factor_panel', df_new_data, mode='skip')
      
      # 6. æŸ¥çœ‹è¡¨ä¿¡æ¯
      lm.get_table_info('factor_panel', print_info=True)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

[é‡è¦æç¤º]

  - æ—¶é—´åˆ—å¿…é¡»å‘½åä¸º 'datetime'ï¼ˆæˆ–è‡ªå®šä¹‰ï¼‰
  - å…¬å¸åˆ—å¿…é¡»å‘½åä¸º 'company'ï¼ˆæˆ–è‡ªå®šä¹‰ï¼‰
  - æ—¶é—´åˆ—å¿…é¡»æ˜¯ pd.to_datetime() è½¬æ¢åçš„æ ¼å¼
  - é‡å¤åˆ¤æ–­åŸºäºï¼ˆæ—¶é—´ï¼Œå…¬å¸ï¼‰ç»„åˆï¼Œè€Œéå•ä¸€ç´¢å¼•

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

[æ›´å¤šä¿¡æ¯]

  - EasyManager å®Œæ•´æ‰‹å†Œï¼šEasyManagerå®Œæ•´ä½¿ç”¨æ‰‹å†Œ.md
  - é•¿æ ¼å¼æ•°æ®è¯´æ˜ï¼šlong_data/README.md
  - å¯¼å…¥ç¤ºä¾‹ï¼šlong_data/import_example.py

â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """
        print(help_text)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºç®¡ç†å™¨å®ä¾‹
    with EasyManager(database='macro_data_base') as em:
        # åˆ—å‡ºæ‰€æœ‰è¡¨
        tables = em.load_table('raw_macro_data_m',limit=-10)
        print("ç°æœ‰è¡¨:", tables)